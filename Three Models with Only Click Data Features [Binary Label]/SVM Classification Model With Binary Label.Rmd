---
title: "SVM Classification Model With Binary Label"
author: "C"
date: "December 5, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

Load Libraries
```{r}
library(ggplot2)
library(kernlab)
library(rpart)
library(ROCR)
library(caret)
library(pROC)
setwd("~/GitHub/Final-Project/Three Models with Only Click Data Features [Binary Label]")
```

Load Data frame and change the formate of the target label to r name format

```{r}
df1 <- read.csv("AAA_Data_Binary_Label.csv")
df1$binary_label <- as.character(df1$binary_label)
df1$binary_label[df1$binary_label == " Monitor"] <- "monitor"
df1$binary_label[df1$binary_label == "Don't Monitor"] <- "ignore"
df1$binary_label <- as.factor(df1$binary_label)

```

Remove X Variable

```{r}

df1 <- df1[,-c(1,2,3,13)]
```

Generate random number to randomize data set

```{r}
gp <- runif(nrow(df1))

```

Mix the rows using the random number as the order

```{r}

df1 <- df1[order(gp),]

```


First, we set up for an analysis, loading the segmentation data set from the caret package and using the caret's createDataPartition() function to produce training and test data sets. Load the data and construct indices to divide it into training and test data sets.

```{r}
trainIndex <- createDataPartition(df1$binary_label,p=.75,list=FALSE)
trainData <- df1[trainIndex,]
testData  <- df1[-trainIndex,]
trainX <-trainData[,-10]        # Pull out the variables for training
sapply(trainX,summary)           # Look at a summary of the training data
```
Next, we carry out a two pass training and tuning process. In the first pass, shown in the code block below, we arbitrarily pick some tuning parameters and use the default caret settings for others. In the trainControl() function we specify 5 repetitions of 10 fold cross validation. in the train() function which actually does the work, we specify the radial kernel using the method parameter and the ROC as the metric for assessing performance. The tuneLength parameter is set to pick 9 arbitrary values for the C, the "cost" of the radial kernel. This parameter controls the complexity of the boundary between support vectors. The radial kernel also requires setting a smoothing parameter, sigma. In this first, pass we let train() use its default method of calculating an analytically derived estimate for sigma. Also note that we instruct train() to center and scale the data before running the analysis with the preProc parameter.


Setup for cross validation

```{r}
set.seed(9850)
ctrl <- trainControl(method="repeatedcv",   # 10fold cross validation
                     repeats=5,		    # do 5 repititions of cv
                     summaryFunction=twoClassSummary,	# Use AUC to pick the best model
                     classProbs=TRUE)
```
Train and Tune the SVM

```{r}
set.seed(9850)
svm.tune <- caret::train(x=trainX,
                  y= trainData$binary_label,
                  method = "svmRadial",   # Radial kernel
                  tuneLength = 9,					# 9 values of the cost function
                  preProc = c("center","scale"),  # Center and scale data
                  metric="ROC",
                  trControl=ctrl)
 
svm.tune


```

The results show that the best model resulted from setting.

 
Tuning parameter 'sigma' was held constant at a value of 0.1430038
ROC was used to select the optimal model using  the largest value.
The final values used for the model were sigma = 0.1440011 and C = 0.25. 

In the second pass, having seen the parameter values selected in the first pass, we use train()'s tuneGrid parameter to do some sensitivity analysis around the values C = .5 and sigma = 0.14 that produced the model with the best ROC value. Note that R's expand.grid() function is used to build a dataframe contain all the combinations of C and sigma we want to look at.

```{r}
# Second pass
# Look at the results of svm.tune and refine the parameter space
 
set.seed(9850)
# Use the expand.grid to specify the search space	
grid <- expand.grid(sigma = c(.0, .15, 0.30),
                    C = c(0.01, 0.10, .25, .35, .50)
)
 
#Train and Tune the SVM
svm.tune <- train(x=trainX,
                    y= trainData$binary_label,
                    method = "svmRadial",
                    preProc = c("center","scale"),
                    metric="ROC",
                    tuneGrid = grid,
                    trControl=ctrl)
 
svm.tune

```
ROC was used to select the optimal model using  the largest value.
The final values used for the model were sigma = 0.15 and C = 0.01. 

```{r}
#Linear Kernel
set.seed(9850)                     
 
#Train and Tune the SVM
svm.tune2 <- train(x=trainX,
                    y= trainData$binary_label,
                    method = "svmLinear",
                    preProc = c("center","scale"),
                    metric="ROC",
                    trControl=ctrl)	
 
 
svm.tune2
```
The following block of code and results shows just the first five lines of the comparison table but includes the summary of the comparison.

```{r}
rValues <- resamples(list(svm=svm.tune,svm.tune2))
rValues$values
summary(rValues)
```
Compare SVM Model kernal

```{r}
bwplot(rValues,metric="ROC",ylab =c("Linear Kernal", "Radial Kernel"))

```


Fit SVM Model 

```{r}
set.seed(9850)   
svmFit <- train(binary_label~.,
    data= trainData,
    method = "svmRadial",
    preProc = c("center", "scale"),
    metric="ROC",
    sigma = .15,
    cost = .01,
    trControl = ctrl)


```
Predict on test set

```{r}
testData$predictedClasses <- predict(svmFit, testData, prob= TRUE)
predictedClasses <- predict(svmFit, testData )
predictedProbs <- predict(svmFit, newdata = testData , type = "prob")
```

Create confusion matrix
```{r}
confusionMatrix(predictedClasses, reference = testData$binary_label)

```
Calculate model performance for ROC 
```{r}
pred_svm <- ROCR::prediction(predictedProbs$monitor, testData$binary_label)
pred_svm <- performance(pred_svm, "tpr", "fpr")
```
Calcualte AUC


```{r}
pred <- predictedProbs[,2]
area <- auc(testData$binary_label, pred)

```

plot ROC

```{r}
rocplot <- function(pred, testData, ...) {
  predob = prediction(pred, testData$binary_label)
  perf = performance(predob, "tpr", "fpr")
  plot(perf, ...)
  area <- format(round(area, 4), nsmall = 4)
  text(x=0.8, y=0.1, labels = paste("AUC =", area))
    segments(x0=0, y0=0, x1=1, y1=1, col="black", lty=2)
}
rocplot(pred, testData, col="black", avg= "threshold", lwd=2, main="Support Vector Machine ROC Curve")

```


